import operator
import json
from typing import Annotated, Literal, Sequence, TypedDict

from langchain_core.utils.function_calling import convert_to_openai_tool
from langgraph.constants import Send
from langgraph.graph import END, START, StateGraph
from langgraph.pregel import Pregel
from openai import OpenAI
from openai.types.chat import ChatCompletionMessageParam

from src.configs import Settings
from src.custom_logger import setup_logger
from src.models import (
    AgentResult,
    Plan,
    ReflectionResult,
    SearchOutput,
    Subtask,
    ToolResult,
)
from src.prompts import HelpDeskAgentPrompts

MAX_CHALLENGE_COUNT = 3

logger = setup_logger(__file__)


class AgentState(TypedDict):
    question: str
    plan: list[str]
    current_step: int
    subtask_results: Annotated[Sequence[Subtask], operator.add]
    last_answer: str


class AgentSubGraphState(TypedDict):
    question: str
    plan: list[str]
    subtask: str
    is_completed: bool
    messages: list[ChatCompletionMessageParam]
    challenge_count: int
    tool_results: Annotated[Sequence[Sequence[SearchOutput]], operator.add]
    reflection_results: Annotated[Sequence[ReflectionResult], operator.add]
    subtask_answer: str


class HelpDeskAgent:
    def __init__(
        self,
        settings: Settings,
        tools: list = [],
        prompts: HelpDeskAgentPrompts = HelpDeskAgentPrompts(),
    ) -> None:
        self.settings = settings
        self.tools = tools
        self.tool_map = {tool.name: tool for tool in tools}
        self.prompts = prompts
        self.client = OpenAI(api_key=self.settings.openai_api_key)

    def create_plan(self, state: AgentState) -> dict:
        """è¨ˆç”»ã‚’ä½œæˆã™ã‚‹

        Args:
            state (AgentState): å…¥åŠ›ã®çŠ¶æ…‹

        Returns:
            AgentState: æ›´æ–°ã•ã‚ŒãŸçŠ¶æ…‹
        """

        logger.info("ğŸš€ Starting plan generation process...")

        # toolå®šç¾©ã‚’æ¸¡ã—ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
        system_prompt = self.prompts.planner_system_prompt

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’æ¸¡ã—ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
        user_prompt = self.prompts.planner_user_prompt.format(
            question=state["question"],
        )
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]
        # logger.info(f"Final prompt messages: {messages}", json.dumps(messages, indent=2, ensure_ascii=False))
        logger.info(
            "INFO Final prompt messages:\n%s",
            json.dumps(messages, indent=2, ensure_ascii=False)
        )


        # OpenAIã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
        try:
            logger.info("Sending request to OpenAI...")
            response = self.client.beta.chat.completions.parse(
                model=self.settings.openai_model,
                messages=messages,
                response_format=Plan,
                temperature=0,
                seed=0,
            )
            logger.info("âœ… Successfully received response from OpenAI.")
        except Exception as e:
            logger.error(f"Error during OpenAI request: {e}")
            raise

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰Structured outputã‚’åˆ©ç”¨ã—Planã‚¯ãƒ©ã‚¹ã‚’å–å¾—
        plan = response.choices[0].message.parsed

        logger.info("Plan generation complete!")

        # ç”Ÿæˆã—ãŸè¨ˆç”»ã‚’è¿”ã—ã€çŠ¶æ…‹ã‚’æ›´æ–°ã™ã‚‹
        return {"plan": plan.subtasks}

    def select_tools(self, state: AgentSubGraphState) -> dict:
        """ãƒ„ãƒ¼ãƒ«ã‚’é¸æŠã™ã‚‹

        Args:
            state (AgentSubGraphState): å…¥åŠ›ã®çŠ¶æ…‹

        Returns:
            dict: æ›´æ–°ã•ã‚ŒãŸçŠ¶æ…‹
        """

        logger.info("ğŸš€ Starting tool selection process...")

        # OpenAIå¯¾å¿œã®toolå®šç¾©ã«æ›¸ãæ›ãˆã‚‹
        logger.info("Converting tools for OpenAI format...")
        openai_tools = [convert_to_openai_tool(tool) for tool in self.tools] # ãƒ¡ãƒ¢ï¼štoolç¾¤ã¯ãŠãŠã‚‚ã¨ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–æ™‚ã«æ³¨å…¥

        # ãƒªãƒˆãƒ©ã‚¤ã•ã‚ŒãŸã‹ã©ã†ã‹ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹
        if state["challenge_count"] == 0:
            logger.info("Creating user prompt for tool selection...")
            user_prompt = self.prompts.subtask_tool_selection_user_prompt.format(
                # ãƒ¡ãƒ¢ï¼šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’çµ„ã¿ç«‹ã¦ã‚‹ã¨ãã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã¯ã“ã‚Œ
                question=state["question"], # ãŠãŠã‚‚ã¨ã®è³ªå•ã‚‚æ¸¡ã—ã¦ã„ã‚‹ï¼
                plan=state["plan"], 
                subtask=state["subtask"],
            )

            messages = [
                {"role": "system", "content": self.prompts.subtask_system_prompt},
                {"role": "user", "content": user_prompt},
                # user_prompt
                    # SUBTASK_TOOL_EXECUTION_USER_PROMPT = """
                    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…ƒã®è³ªå•: {question}
                    # å›ç­”ã®ãŸã‚ã®è¨ˆç”»: {plan}
                    # ã‚µãƒ–ã‚¿ã‚¹ã‚¯: {subtask}

                    # ã‚µãƒ–ã‚¿ã‚¹ã‚¯å®Ÿè¡Œã‚’é–‹å§‹ã—ã¾ã™ã€‚
                    # 1.ãƒ„ãƒ¼ãƒ«é¸æŠãƒ»å®Ÿè¡Œ, 2ã‚µãƒ–ã‚¿ã‚¹ã‚¯å›ç­”ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„
                    # """

                # SUBTASK_SYSTEM_PROMPT = """
                #     ã‚ãªãŸã¯XYZã¨ã„ã†ã‚·ã‚¹ãƒ†ãƒ ã®è³ªå•å¿œç­”ã®ãŸã‚ã«ã‚µãƒ–ã‚¿ã‚¹ã‚¯å®Ÿè¡Œã‚’æ‹…å½“ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚
                #     å›ç­”ã¾ã§ã®å…¨ä½“ã®æµã‚Œã¯è¨ˆç”»ç«‹æ¡ˆ â†’ ã‚µãƒ–ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ [ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œ â†’ ã‚µãƒ–ã‚¿ã‚¹ã‚¯å›ç­” â†’ ãƒªãƒ•ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³] â†’ æœ€çµ‚å›ç­”ã¨ãªã‚Šã¾ã™ã€‚
                #     ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å›ç­”ã™ã‚‹ãŸã‚ã«è€ƒãˆã‚‰ã‚ŒãŸè¨ˆç”»ã®ä¸€ã¤ã§ã™ã€‚
                #     æœ€çµ‚çš„ãªå›ç­”ã¯å…¨ã¦ã®ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã®çµæœã‚’çµ„ã¿åˆã‚ã›ã¦åˆ¥ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒä½œæˆã—ã¾ã™ã€‚
                #     ã‚ãªãŸã¯ä»¥ä¸‹ã®1~3ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’æŒ‡ç¤ºã«å¾“ã£ã¦ãã‚Œãã‚Œå®Ÿè¡Œã—ã¾ã™ã€‚å„ã‚¹ãƒ†ãƒƒãƒ—ã¯æŒ‡ç¤ºãŒã‚ã£ãŸã‚‰å®Ÿè¡Œã—ã€åŒæ™‚ã«è¤‡æ•°ã‚¹ãƒ†ãƒƒãƒ—ã®å®Ÿè¡Œã¯è¡Œã‚ãªã„ã§ãã ã•ã„ã€‚
                #     ãªãŠãƒªãƒ•ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®çµæœæ¬¡ç¬¬ã§æ‰€å®šã®å›æ•°ã¾ã§ãƒ„ãƒ¼ãƒ«é¸æŠãƒ»å®Ÿè¡Œã‚’ç¹°ã‚Šè¿”ã—ã¾ã™ã€‚

                #     1. ãƒ„ãƒ¼ãƒ«é¸æŠãƒ»å®Ÿè¡Œ
                #     ã‚µãƒ–ã‚¿ã‚¹ã‚¯å›ç­”ã®ãŸã‚ã®ãƒ„ãƒ¼ãƒ«é¸æŠã¨é¸æŠã•ã‚ŒãŸãƒ„ãƒ¼ãƒ«ã®å®Ÿè¡Œã‚’è¡Œã„ã¾ã™ã€‚
                #     2å›ç›®ä»¥é™ã¯ãƒªãƒ•ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã«å¾“ã£ã¦å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚

                #     2. ã‚µãƒ–ã‚¿ã‚¹ã‚¯å›ç­”
                #     ãƒ„ãƒ¼ãƒ«ã®å®Ÿè¡Œçµæœã¯ã‚ãªãŸã—ã‹è¦³æ¸¬ã§ãã¾ã›ã‚“ã€‚
                #     ãƒ„ãƒ¼ãƒ«ã®å®Ÿè¡Œçµæœã‹ã‚‰å¾—ã‚‰ã‚ŒãŸå›ç­”ã«å¿…è¦ãªã“ã¨ã¯è¨€èªåŒ–ã—ã€æœ€å¾Œã®å›ç­”ç”¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«å¼•ãç¶™ã’ã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚
                #     ä¾‹ãˆã°ã€æ¦‚è¦ã‚’çŸ¥ã‚‹ã‚µãƒ–ã‚¿ã‚¹ã‚¯ãªã‚‰ã°ã€ãƒ„ãƒ¼ãƒ«ã®å®Ÿè¡Œçµæœã‹ã‚‰æ¦‚è¦ã‚’è¨€èªåŒ–ã—ã¦ãã ã•ã„ã€‚
                #     æ‰‹é †ã‚’çŸ¥ã‚‹ã‚µãƒ–ã‚¿ã‚¹ã‚¯ãªã‚‰ã°ã€ãƒ„ãƒ¼ãƒ«ã®å®Ÿè¡Œçµæœã‹ã‚‰æ‰‹é †ã‚’è¨€èªåŒ–ã—ã¦ãã ã•ã„ã€‚
                #     å›ç­”ã§ããªã‹ã£ãŸå ´åˆã¯ã€ãã®æ—¨ã‚’è¨€èªåŒ–ã—ã¦ãã ã•ã„ã€‚

                #     3. ãƒªãƒ•ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
                #     ãƒ„ãƒ¼ãƒ«ã®å®Ÿè¡Œçµæœã¨å›ç­”ã‹ã‚‰ã€ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã«å¯¾ã—ã¦æ­£ã—ãå›ç­”ã§ãã¦ã„ã‚‹ã‹ã‚’è©•ä¾¡ã—ã¾ã™ã€‚
                #     å›ç­”ãŒã‚ã‹ã‚‰ãªã„ã€æƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã¨ã„ã£ãŸå†…å®¹ã®å ´åˆã¯è©•ä¾¡ã‚’NGã«ã—ã€ã‚„ã‚Šç›´ã™ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚
                #     è©•ä¾¡ãŒNGã®å ´åˆã¯ã€åˆ¥ã®ãƒ„ãƒ¼ãƒ«ã‚’è©¦ã™ã€åˆ¥ã®æ–‡è¨€ã§ãƒ„ãƒ¼ãƒ«ã‚’è©¦ã™ãªã©ã€ãªãœNGãªã®ã‹ã¨ã©ã†ã—ãŸã‚‰æ”¹å–„ã§ãã‚‹ã‹ã‚’è€ƒãˆã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
                #     ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã®å†…å®¹ã¯éå»ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã¨è¨ˆç”»å†…ã®ä»–ã®ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã¨é‡è¤‡ã—ãªã„ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚
                #     ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã®å†…å®¹ã‚’ã‚‚ã¨ã«ãƒ„ãƒ¼ãƒ«é¸æŠãƒ»å®Ÿè¡Œã‹ã‚‰ã‚„ã‚Šç›´ã—ã¾ã™ã€‚
                #     è©•ä¾¡ãŒOKã®å ´åˆã¯ã€ã‚µãƒ–ã‚¿ã‚¹ã‚¯å›ç­”ã‚’çµ‚äº†ã—ã¾ã™ã€‚

                #     """
            ]

        else:
            logger.info("Creating user prompt for tool retry...")

            # ãƒªãƒˆãƒ©ã‚¤ã•ã‚ŒãŸå ´åˆã¯éå»ã®å¯¾è©±æƒ…å ±ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ ã™ã‚‹
            messages: list = state["messages"]

            # NOTE: ãƒˆãƒ¼ã‚¯ãƒ³æ•°ç¯€ç´„ã®ãŸã‚éå»ã®æ¤œç´¢çµæœã¯é™¤ã
            # roleãŒtoolã¾ãŸã¯tool_callsã‚’æŒã¤ã‚‚ã®ã¯é™¤ã
            messages = [message for message in messages if message["role"] != "tool" or "tool_calls" not in message]

            user_retry_prompt = self.prompts.subtask_retry_answer_user_prompt
            user_message = {"role": "user", "content": user_retry_prompt}
            messages.append(user_message)

        try:
            logger.info("Sending request to OpenAI...")
            response = self.client.chat.completions.create(
                model=self.settings.openai_model,
                messages=messages,
                tools=openai_tools,  # type: ignore
                temperature=0,
                seed=0,
            )
            logger.info(response)

            # logger.info("âœ… Successfully received response from OpenAI.")
        except Exception as e:
            logger.error(f"Error during OpenAI request: {e}")
            raise

        if response.choices[0].message.tool_calls is None:
            raise ValueError("Tool calls are None")

        ai_message = {
            "role": "assistant",
            "tool_calls": [tool_call.model_dump() for tool_call in response.choices[0].message.tool_calls],
        }

        logger.info("Tool selection complete!")
        messages.append(ai_message)

        # ãƒªãƒˆãƒ©ã‚¤ã®å ´åˆã¯è¿½åŠ åˆ†ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿ã‚’æ›´æ–°ã™ã‚‹
        return {"messages": messages}

    def execute_tools(self, state: AgentSubGraphState) -> dict:
        """ãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹

        Args:
            state (AgentSubGraphState): å…¥åŠ›ã®çŠ¶æ…‹

        Raises:
            ValueError: toolãŒNoneã®å ´åˆ

        Returns:
            dict: æ›´æ–°ã•ã‚ŒãŸçŠ¶æ…‹
        """

        logger.info("ğŸš€ Starting tool execution process...")
        messages = state["messages"]

        # æœ€å¾Œã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰ãƒ„ãƒ¼ãƒ«ã®å‘¼ã³å‡ºã—ã‚’å–å¾—
        tool_calls = messages[-1]["tool_calls"]

        # æœ€å¾Œã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‹ã‚‰ãƒ„ãƒ¼ãƒ«ã®å‘¼ã³å‡ºã—ã‹ç¢ºèª
        if tool_calls is None:
            logger.error("Tool calls are None")
            logger.error(f"Messages: {messages}")
            raise ValueError("Tool calls are None")

        tool_results = []

        for tool_call in tool_calls:
            tool_name = tool_call["function"]["name"]
            tool_args = tool_call["function"]["arguments"]

            tool = self.tool_map[tool_name]
            tool_result: list[SearchOutput] = tool.invoke(tool_args)

            tool_results.append(
                ToolResult(
                    tool_name=tool_name,
                    args=tool_args,
                    results=tool_result,
                )
            )

            messages.append(
                {
                    "role": "tool",
                    "content": str(tool_result),
                    "tool_call_id": tool_call["id"],
                }
            )
        logger.info("Tool execution complete!")
        return {"messages": messages, "tool_results": [tool_results]}

    def create_subtask_answer(self, state: AgentSubGraphState) -> dict:
        """ã‚µãƒ–ã‚¿ã‚¹ã‚¯å›ç­”ã‚’ä½œæˆã™ã‚‹

        Args:
            state (AgentSubGraphState): å…¥åŠ›ã®çŠ¶æ…‹

        Returns:
            dict: æ›´æ–°ã•ã‚ŒãŸçŠ¶æ…‹
        """

        logger.info("ğŸš€ Starting subtask answer creation process...")
        messages = state["messages"]

        try:
            logger.info("Sending request to OpenAI...")
            response = self.client.chat.completions.create(
                model=self.settings.openai_model,
                messages=messages,
                temperature=0,
                seed=0,
            )
            logger.info("âœ… Successfully received response from OpenAI.")
        except Exception as e:
            logger.error(f"Error during OpenAI request: {e}")
            raise

        subtask_answer = response.choices[0].message.content

        ai_message = {"role": "assistant", "content": subtask_answer}
        messages.append(ai_message)

        logger.info("Subtask answer creation complete!")

        return {
            "messages": messages,
            "subtask_answer": subtask_answer,
        }

    def reflect_subtask(self, state: AgentSubGraphState) -> dict:
        """ã‚µãƒ–ã‚¿ã‚¹ã‚¯å›ç­”ã‚’å†…çœã™ã‚‹

        Args:
            state (AgentSubGraphState): å…¥åŠ›ã®çŠ¶æ…‹

        Raises:
            ValueError: reflection resultãŒNoneã®å ´åˆ

        Returns:
            dict: æ›´æ–°ã•ã‚ŒãŸçŠ¶æ…‹
        """

        logger.info("ğŸš€ Starting reflection process...")
        messages = state["messages"]

        user_prompt = self.prompts.subtask_reflection_user_prompt

        messages.append({"role": "user", "content": user_prompt})

        try:
            logger.info("Sending request to OpenAI...")
            response = self.client.beta.chat.completions.parse( # æ§‹é€ åŒ–ã—ãŸã„ã¨ãï¼ˆresponse_formatï¼‰
                model=self.settings.openai_model,
                messages=messages,
                response_format=ReflectionResult,
                temperature=0,
                seed=0,
            )
            logger.info("âœ… Successfully received response from OpenAI.")
        except Exception as e:
            logger.error(f"Error during OpenAI request: {e}")
            raise

        reflection_result = response.choices[0].message.parsed
        if reflection_result is None:
            raise ValueError("Reflection result is None")

        messages.append(
            {
                "role": "assistant",
                "content": reflection_result.model_dump_json(),
            }
        )

        update_state = {
            "messages": messages,
            "reflection_results": [reflection_result],
            "challenge_count": state["challenge_count"] + 1,
            "is_completed": reflection_result.is_completed,
        }

        if update_state["challenge_count"] >= MAX_CHALLENGE_COUNT and not reflection_result.is_completed:
            update_state["subtask_answer"] = f"{state['subtask']}ã®å›ç­”ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"

        logger.info("Reflection complete!")
        return update_state

    def create_answer(self, state: AgentState) -> dict:
        """æœ€çµ‚å›ç­”ã‚’ä½œæˆã™ã‚‹

        Args:
            state (AgentState): å…¥åŠ›ã®çŠ¶æ…‹

        Returns:
            dict: æ›´æ–°ã•ã‚ŒãŸçŠ¶æ…‹
        """

        logger.info("ğŸš€ Starting final answer creation process...")
        system_prompt = self.prompts.create_last_answer_system_prompt

        # ã‚µãƒ–ã‚¿ã‚¹ã‚¯çµæœã®ã†ã¡ã‚¿ã‚¹ã‚¯å†…å®¹ã¨å›ç­”ã®ã¿ã‚’å–å¾—
        subtask_results = [(result.task_name, result.subtask_answer) for result in state["subtask_results"]]
        user_prompt = self.prompts.create_last_answer_user_prompt.format(
            question=state["question"],
            plan=state["plan"],
            subtask_results=str(subtask_results),
        )
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        try:
            logger.info("Sending request to OpenAI...")
            response = self.client.chat.completions.create(
                model=self.settings.openai_model,
                messages=messages,
                temperature=0,
                seed=0,
            )
            logger.info("âœ… Successfully received response from OpenAI.")
        except Exception as e:
            logger.error(f"Error during OpenAI request: {e}")
            raise

        logger.info("Final answer creation complete!")

        return {"last_answer": response.choices[0].message.content}

    def _execute_subgraph(self, state: AgentState):
        subgraph = self._create_subgraph()

        dict_input =  {
            "question": state["question"],# è¦ªã‚°ãƒ©ãƒ•ã®questionã‚’ãã®ã¾ã¾
            "plan": state["plan"], # è¦ªã‚°ãƒ©ãƒ•ã®planã‚’ãã®ã¾ã¾ã‚‚ã£ã¦ããŸå½¢
            "subtask": state["plan"][state["current_step"]], # ã“ã“ã¯planãŒã©ã†ã„ã†æ›´æ–°ã‚’ã—ã¦ã„ã‚‹ã®ã‹ã‚’è¦‹ã‚‹å¿…è¦ãŒã‚ã‚‹ãª
            "current_step": state["current_step"],
            "is_completed": False,
            "challenge_count": 0,
        }

        logger.info(
            "INFO Final prompt messages:\n%s",
            json.dumps(dict_input, indent=2, ensure_ascii=False)
        )

        result = subgraph.invoke(dict_input)
        


        subtask_result = Subtask(
            task_name=result["subtask"],
            tool_results=result["tool_results"],
            reflection_results=result["reflection_results"],
            is_completed=result["is_completed"],
            subtask_answer=result["subtask_answer"],
            challenge_count=result["challenge_count"],
        )

        return {"subtask_results": [subtask_result]}

    def _should_continue_exec_subtasks(self, state: AgentState) -> list:
        return [
            Send(
                "execute_subtasks",
                {
                    "question": state["question"],
                    "plan": state["plan"],
                    "current_step": idx,
                },
            )
            for idx, _ in enumerate(state["plan"])
        ]

    def _should_continue_exec_subtask_flow(self, state: AgentSubGraphState) -> Literal["end", "continue"]:
        if state["is_completed"] or state["challenge_count"] >= MAX_CHALLENGE_COUNT:
            return "end"
        else:
            return "continue"

    def _create_subgraph(self) -> Pregel:
        """ã‚µãƒ–ã‚°ãƒ©ãƒ•ã‚’ä½œæˆã™ã‚‹

        Returns:
            Pregel: ã‚µãƒ–ã‚°ãƒ©ãƒ•
        """
        workflow = StateGraph(AgentSubGraphState)

        # ãƒ„ãƒ¼ãƒ«é¸æŠãƒãƒ¼ãƒ‰ã‚’è¿½åŠ 
        workflow.add_node("select_tools", self.select_tools)

        # ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œãƒãƒ¼ãƒ‰ã‚’è¿½åŠ 
        workflow.add_node("execute_tools", self.execute_tools)

        # ã‚µãƒ–ã‚¿ã‚¹ã‚¯å›ç­”ä½œæˆãƒãƒ¼ãƒ‰ã‚’è¿½åŠ 
        workflow.add_node("create_subtask_answer", self.create_subtask_answer)

        # ã‚µãƒ–ã‚¿ã‚¹ã‚¯å†…çœãƒãƒ¼ãƒ‰ã‚’è¿½åŠ 
        workflow.add_node("reflect_subtask", self.reflect_subtask)

        # ãƒ„ãƒ¼ãƒ«é¸æŠã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆ
        workflow.add_edge(START, "select_tools")

        # ãƒãƒ¼ãƒ‰é–“ã®ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ 
        workflow.add_edge("select_tools", "execute_tools")
        workflow.add_edge("execute_tools", "create_subtask_answer")
        workflow.add_edge("create_subtask_answer", "reflect_subtask")

        # ã‚µãƒ–ã‚¿ã‚¹ã‚¯å†…çœãƒãƒ¼ãƒ‰ã®çµæœã‹ã‚‰ç¹°ã‚Šè¿”ã—ã®ãŸã‚ã®ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ 
        workflow.add_conditional_edges(
            "reflect_subtask",
            self._should_continue_exec_subtask_flow,
            {"continue": "select_tools", "end": END},
        )

        app = workflow.compile()

        return app

    def create_graph(self) -> Pregel:
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ¡ã‚¤ãƒ³ã‚°ãƒ©ãƒ•ã‚’ä½œæˆã™ã‚‹

        Returns:
            Pregel: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ¡ã‚¤ãƒ³ã‚°ãƒ©ãƒ•
        """
        workflow = StateGraph(AgentState)

        # Add the plan node
        workflow.add_node("create_plan", self.create_plan) # AgentStateå…¨ä½“ã‚’è¿”ã—ã¦ã‚‚ã‚ˆã„ã—ã€è¿½è¨˜ã—ãŸã„éƒ¨åˆ†ã®ã¿ã‚’æã„ã¦ã‚‚ã‚ˆã„ã‚‰ã—ã„

        # Add the execution step
        workflow.add_node("execute_subtasks", self._execute_subgraph)

        workflow.add_node("create_answer", self.create_answer)

        workflow.add_edge(START, "create_plan")

        # From plan we go to agent
        workflow.add_conditional_edges(
            "create_plan",
            self._should_continue_exec_subtasks,
        )

        # From agent, we replan
        workflow.add_edge("execute_subtasks", "create_answer")

        workflow.set_finish_point("create_answer")

        app = workflow.compile()

        return app

    def run_agent(self, question: str) -> AgentResult:
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡Œã™ã‚‹

        Args:
            question (str): å…¥åŠ›ã®è³ªå•

        Returns:
            AgentResult: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè¡Œçµæœ
        """

        app = self.create_graph()
        result = app.invoke(
            {
                "question": question, # å•ã„åˆã‚ã›å†…å®¹ãã®ã‚‚ã®ï¼ˆUIã«å…¥åŠ›ã•ã‚ŒãŸãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒªï¼‰
                "current_step": 0,
            }
        )
        return AgentResult(
            question=question,
            plan=Plan(subtasks=result["plan"]),
            subtasks=result["subtask_results"],
            answer=result["last_answer"],
        )
